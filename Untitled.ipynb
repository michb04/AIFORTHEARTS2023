{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d86700-f13f-41f3-b5b4-8a6e2f37292d",
   "metadata": {},
   "source": [
    "# Critically Engaging with AI Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5ff66-57f4-4432-9375-722276468530",
   "metadata": {},
   "source": [
    " Task 2: Identifying Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772105fc-e2d7-4405-8abd-07022b7f7b71",
   "metadata": {},
   "source": [
    "### Task 2: Identifying Bias\n",
    "\n",
    "#### Task 2-a: Understanding the Scope of Bias\n",
    "\n",
    "**Types of Bias Described:**\n",
    "1. Selection Bias\n",
    "2. Measurement Bias\n",
    "3. Algorithmic Bias\n",
    "4. Confirmation Bias\n",
    "5. Label Bias\n",
    "\n",
    "**Known Biases:**\n",
    "- Selection Bias\n",
    "- Confirmation Bias\n",
    "\n",
    "**New Biases:**\n",
    "- Measurement Bias\n",
    "- Algorithmic Bias\n",
    "- Label Bias\n",
    "\n",
    "**Additional Biases:**\n",
    "- Survivorship Bias\n",
    "- Sampling Bias\n",
    "\n",
    "#### Task 2-b: Exploring Bias in the Jigsaw Toxic Comment Classification Challenge\n",
    "\n",
    "**Findings:**\n",
    "- **Identified Biases:** \n",
    "  - Selection Bias: Certain types of comments might be underrepresented.\n",
    "  - Label Bias: Human annotators might have inconsistent labeling standards.\n",
    "  - Algorithmic Bias: The model might perform differently across various demographic groups.\n",
    "\n",
    "**Discussion:**\n",
    "- **Key Points:**\n",
    "  - It's important to identify and mitigate biases at every stage of the machine learning pipeline.\n",
    "  - Awareness and continuous monitoring are crucial for ensuring fairness and accuracy.\n",
    "- **Mitigation Strategies:**\n",
    "  - Diversifying the dataset to include a wide range of comment types.\n",
    "  - Standardising labeling procedures to reduce label bias.\n",
    "  - Regularly evaluating model performance across different subgroups to detect algorithmic bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15821e-e8f8-43e4-b608-42a73c89f154",
   "metadata": {},
   "source": [
    "### Task 3: Large Language Models and Bias: Word Embedding Demo\n",
    "\n",
    "#### Task 3.1: Initial Exploration of Words and Relationships\n",
    "\n",
    "- **Apple:**\n",
    "  - **Observation:** Words like \"computer\" and \"mac\" are close to \"apple\" \n",
    "- **Silver:**\n",
    "  - **Observation:** Words like \"gold,\" \"copper,\" \"iron,\" and \"bronze.\" are close to \"silver\"\n",
    "- **Sound:**\n",
    "  - **Observation:** Words like \"sounds\" and \"soundtrack\" are close to \"sound.\"\n",
    "\n",
    "**Conclusion:** Words related to each other are generally situated closer together in the Word2Vec model.\n",
    "\n",
    "#### Task 3.2: Exploring \"Word2Vec All\" for Patterns\n",
    "\n",
    "\n",
    "- **Engineer:**\n",
    "  - **Observation:** Related words include \"engineering,\" \"inventors,\" \"mathematicians,\" \"physicists,\" and \"designers.\"\n",
    "- **Drummer:**\n",
    "  - **Observation:** Related words include \"guitarist,\" \"bassist,\" \"musician,\" \"singer,\" and \"songwriter.\"\n",
    "\n",
    "#### Discussion on Gender Bias\n",
    "\n",
    "- **Observation:** There are potential concerns of gender bias. For example, words related to \"engineer\" are predominantly associated with traditionally male-dominated professions, whereas words related to \"drummer\" are more gender-neutral but still tend to cluster around traditionally male-associated roles in music.\n",
    "- **Conclusion:** This suggests that gender biases may be present in the word embeddings, reflecting societal biases in occupational roles.\n",
    "\n",
    "#### Attribution\n",
    "\n",
    "- The embeddings and visualizations are from the [TensorFlow Embedding Projector](https://projector.tensorflow.org/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314cb08-3c00-4faf-bf04-e370fa1b3898",
   "metadata": {},
   "source": [
    "### Task 4: Thinking about AI Fairness\n",
    "\n",
    "#### Task 4-a: Topics in AI Fairness\n",
    "\n",
    "- **Criteria Described:**\n",
    "  - **Demographic Parity / Statistical Parity:**\n",
    "    - Ensures the model selects people in proportions that match the group membership percentages of the applicants.\n",
    "    - Example: A conference selecting speakers with a model that ensures 50% of selected candidates are women if 50% of the attendees are women.\n",
    "  - **Equal Opportunity:**\n",
    "    - Ensures the true positive rate (TPR) is equal for each group.\n",
    "    - Example: A medical tool designed to have a high TPR that is equal for each demographic group.\n",
    "  - **Equal Accuracy:**\n",
    "    - Ensures the model has the same percentage of correct classifications for each group.\n",
    "    - Example: A bank loan approval model that is equally accurate for all demographic groups.\n",
    "  - **Group Unaware / \"Fairness through Unawareness\":**\n",
    "    - Removes all group membership information from the dataset to prevent bias based on these groups.\n",
    "    - Example: Removing gender, race, or age data from a model, but also ensuring proxies like zip code are removed to avoid inference of group membership.\n",
    "\n",
    "- **Known and New Criteria:**\n",
    "  - Criteria I already knew about before this course:\n",
    "    - Group unaware / \"Fairness through unawareness\"\n",
    "  - Criteria that were new to me:\n",
    "    - Demographic parity / statistical parity\n",
    "    - Equal opportunity\n",
    "    - Equal accuracy\n",
    "\n",
    "- **Additional Criteria:**\n",
    "  - Transparency in model decision-making\n",
    "  - Accountability in AI deployment\n",
    "  - Continuous monitoring and updating of AI models to ensure fairness\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Task 4-b: AI Fairness in the Context of the Credit Card Dataset\n",
    "\n",
    "\n",
    "**Key Points and Findings:**\n",
    "\n",
    "1. **Baseline Model:**\n",
    "   - **Performance:**\n",
    "     - Total approvals: 38,246\n",
    "     - Group A approvals: 8,028 (21%)\n",
    "     - Group B approvals: 30,218 (79%)\n",
    "     - Overall accuracy: 94.89%\n",
    "     - Group A accuracy: 94.56%\n",
    "     - Group B accuracy: 95.02%\n",
    "     - True positive rate (TPR) for Group A: 77.23%\n",
    "     - TPR for Group B: 98.03%\n",
    "   - **Observations:**\n",
    "     - Group B had a higher representation in approved applicants.\n",
    "     - Higher TPR for Group B indicates an unfair advantage in model approval decisions.\n",
    "\n",
    "2. **Group Unaware Model:**\n",
    "   - **Performance:**\n",
    "     - Overall, removing group information resulted in a model that did not significantly reduce the disparity in fairness metrics.\n",
    "   - **Observations:**\n",
    "     - While removing group membership information aimed to eliminate bias, proxies within the data may still perpetuate inequality.\n",
    "     - Demographic parity, equal accuracy, and equal opportunity were not fully achieved.\n",
    "\n",
    "3. **Demographic Parity Model:**\n",
    "   - **Performance:**\n",
    "     - Adjusted thresholds to balance approval rates between groups.\n",
    "     - This approach showed improved demographic parity but may have affected other fairness criteria such as accuracy and opportunity.\n",
    "   - **Observations:**\n",
    "     - Balancing representation in approved applicants was partially successful.\n",
    "     - Trade-offs between different fairness criteria were evident.\n",
    "\n",
    "**Discussion and Reflections:**\n",
    "\n",
    "- **Fairness Criteria:**\n",
    "  - **Demographic Parity:** Group B had an unfair advantage in the baseline model. Adjusting thresholds improved this but introduced trade-offs.\n",
    "  - **Equal Accuracy:** The model was slightly more accurate for Group B, leading to potential unfairness.\n",
    "  - **Equal Opportunity:** Group B had a significantly higher TPR, highlighting a bias in positive classifications.\n",
    "\n",
    "- **Model Fairness:**\n",
    "  - The fairness of models is complex and context-dependent. Removing group membership alone does not ensure fairness due to hidden proxies.\n",
    "  - Ensuring fairness requires a balance between various criteria and careful consideration of the data and context.\n",
    "\n",
    "**Conclusion:**\n",
    "Achieving fairness in AI models is a nuanced and ongoing challenge. It involves not only model training but also understanding and mitigating biases in data collection and preprocessing. This exercise highlights the importance of evaluating multiple fairness criteria and recognising the trade-offs involved in striving for equitable AI systems.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4daaea-60af-4f58-91ff-3a9a6e64b907",
   "metadata": {},
   "source": [
    "### Task 5: AI and Explainability\n",
    "\n",
    "**Introduction to Permutation Importance:**\n",
    "Permutation importance is a technique used to determine the importance of features in a machine learning model by measuring the change in model performance when the values of a feature are shuffled. If shuffling the values of a feature significantly decreases model performance, that feature is considered important.\n",
    "\n",
    "**Exercise Overview:**\n",
    "We followed the tutorial on Permutation Importance at Kaggle, which provided an example of predicting whether a soccer team will have the \"Man of the Game\" winner based on team statistics. We then moved on to a hands-on exercise involving a Taxi Fare Prediction dataset.\n",
    "\n",
    "**Task 1-a: Analysis of Taxi Fare Prediction Dataset**\n",
    "\n",
    "1. **Number of Features:**\n",
    "   - The dataset used in the exercise contains several features, including:\n",
    "     - pickup_latitude\n",
    "     - pickup_longitude\n",
    "     - dropoff_latitude\n",
    "     - dropoff_longitude\n",
    "     - passenger_count\n",
    "     - pickup_hour\n",
    "     - pickup_minute\n",
    "     - pickup_day_of_week\n",
    "\n",
    "2. **Results and Intuition:**\n",
    "   - **Results:** The permutation importance revealed that features like pickup_longitude, dropoff_longitude, and pickup_latitude had high importance, while features like passenger_count and pickup_minute were less important.\n",
    "   - **Contrary to Intuition?:** The results were somewhat intuitive. Geographic features (pickup and dropoff locations) are naturally expected to be highly predictive of taxi fare due to distance and location-based pricing. However, some might intuitively expect passenger_count to have more importance, which was not the case here, likely because fare primarily depends on distance and time rather than the number of passengers.\n",
    "\n",
    "3. **Discussion with Peers:**\n",
    "   - Our peer discussion highlighted that while the results aligned with general expectations about fare prediction, they also underscored the value of empirical feature importance assessment. It was noted that certain intuitions, like the perceived importance of passenger_count, might not always hold true in practice.\n",
    "\n",
    "\n",
    "\n",
    "**Task 1-b: Reflecting on Permutation Importance**\n",
    "\n",
    "1. **Reasonableness of Permutation Importance:**\n",
    "   - Permutation importance is a reasonable measure of feature importance because it directly evaluates the impact of each feature on model performance. It provides a straightforward and interpretable way to understand feature contributions.\n",
    "\n",
    "2. **Potential Issues with Permutation Importance:**\n",
    "   - **Correlated Features:** If features are highly correlated, shuffling one feature may not significantly impact model performance because the correlated feature can still provide similar information.\n",
    "   - **Non-Linear Relationships:** In models capturing complex, non-linear relationships, the impact of shuffling may not fully capture the importance of interactions between features.\n",
    "   - **Data Sensitivity:** The technique is sensitive to the specific data used for shuffling. Different subsets of data might yield different importance rankings.\n",
    "\n",
    "**Example Issue:**\n",
    "   - Consider a model predicting house prices with features like square footage and number of bedrooms. These features are often correlated; shuffling square footage might not drastically affect the model if the number of bedrooms remains in place, potentially underestimating the importance of square footage.\n",
    "\n",
    "**Discussion:**\n",
    "   - Despite these issues, permutation importance remains a valuable tool for feature importance assessment, particularly when used alongside other methods. It encourages data scientists to critically evaluate model behavior and ensure robustness in their feature selection process.\n",
    "\n",
    "**Conclusion:**\n",
    "Permutation importance provides a valuable lens through which to view and understand feature importance in machine learning models. By highlighting both intuitive and non-intuitive results, it aids in refining models and enhancing their interpretability and fairness.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdeeb12-c786-477f-b1b6-5d6533c7882c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
